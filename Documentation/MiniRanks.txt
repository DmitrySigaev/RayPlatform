Mini-ranks
SÃ©bastien Boisvert
2012-10-17, Argonne National Laboratory

This idea was discussed during a meeting with Professor Rick Stevens.

The concept of mini-rank in electronics was introduced in 2008 in the paper

"Mini-rank: Adaptive DRAM architecture for improving memory power efficiency"
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.182.4308&rep=rep1&type=pdf


Here, we use the concept in the context of MPI ranks, not memory ranks.

The processing machinery of a computer is hierarchical with
this progression:

	socket -> processor -> core -> thread

The MPI programming model only gives MPI ranks that send messages to each
other. Usually a MPI Rank is mapped to a single process. And a process runs
on a single core.

Figure 1: The MPI programming model.

            +--------------------+
	    |   MPI_COMM_WORLD   |           MPI communicator
            +---------+----------+
	              |
    +------+------+---+--+------+------+
    |      |      |      |      |      |
  +---+  +---+  +---+  +---+  +---+  +---+ 
  | 0 |  | 1 |  | 2 |  | 3 |  | 4 |  | 5 |    MPI ranks
  +---+  +---+  +---+  +---+  +---+  +---+


Mini ranks can be thought as ranks within ranks.


Figure 2: The MPI programming model, with mini ranks.

            +--------------------+
	    |   MPI_COMM_WORLD   |           MPI communicator
            +---------+----------+
	              |
    +------+------+---+--+------+------+
    |      |      |      |      |      |
  +---+  +---+  +---+  +---+  +---+  +---+ 
  | 0 |  | 1 |  | 2 |  | 3 |  | 4 |  | 5 |    MPI ranks   (1 VirtualMachine.cpp instance per rank)
  +---+  +---+  +---+  +---+  +---+  +---+                    with the main for MPI calls
  |   |  |   |  |   |  |   |  |   |  |   |
  | 0 |  | 4 |  | 8 |  |12 |  |16 |  |20 |  |                
  | 1 |  | 5 |  | 9 |  |13 |  |17 |  |21 |  | => mini ranks
  | 2 |  | 6 |  |10 |  |14 |  |18 |  |22 |  |
  | 3 |  | 7 |  |11 |  |15 |  |19 |  |23 |  |  (1 Minirank instance per minirank (in 1 pthread))
  |   |  |   |  |   |  |   |  |   |  |   |
  +---+  +---+  +---+  +---+  +---+  +---+       (will wrap Machine.cpp and ComputeCore.cpp)

Current programming model:

Each MPI rank runs one 

class Machine.cpp (in Ray)
this class create application plugins and starts the main loop

the main loop is in RayPlatform.




Future software design:


class VirtualMachine.cpp  (runs as 1 MPI rank)

             (1 pthread doing the MPI calls)
		- picks up stuff from mini rank outbox
		- copy these buffers into another one that will be used by
		Mailman
		- deposit incoming messages into the inbox of miniranks


	class Machine.cpp  (running a mini rank in 1 pthread)
		plugins see only a inbox and a outbox

		buffers of mini ranks are pristine, they can not be dirty
		they can also call barrier on RayPlatform

		class ComputeCore.cpp  (with the main loop)

			- add locks on inbox and outbox.


As of Ray v2.0.0, each MPI rank is running as a Machine.cpp

Machine.cpp is not in RayPlatform however.


## Adapters

For adapters, the current implementation is that each plugin is a singleton.
This must be changed. Instead, each static variable in a plugin must be an
array of plugin.

Then, the callbacks will take an extra parameter: the index of the minirank
within the rank.

The current code was reverted to the virtual methods.


## Message reception

Designed in collaboration with Fangfang Xia

1. rank receives message with MPI
2. rank locks the corresponding minirank inbox
3. rank push the message to the minirank inbox
4. rank unlocks the minirank inbox
5. The minirank locks its inbox
6. The minirank consumes its message
7. The minirank unlocks its inbox

## Sending a message

Designed in collaboration with Fangfang Xia

1. The minirank puts a message in its outbox
2. The minirank locks the rank outbox
3. The minirank pushes onto the rank outbox
4. The minirank unlocks the rank outbox
5. The rank locks its outbox
6. The rank pushes messages onto the network
7. The rank unlocks its outbox



1 important aspect is that the rank has no buffering system at all.
All the buffering systems are in the miniranks. For MPI_Recv, the virtual
machine uses the buffer from the corresponding minirank. 
For MPI_Isend, the virtual machine uses the buffer from the minirank.
The dirty/cleaning subsystem is local to each minirank.


For a system with 8 nodes, 2 processors per node, and with 4 cores per processor,
we have 16 processors and a total of 64 cores.

Case 1:
It can be run like with the classic MPI programming model using 1 MPI rank per
core -- 64 MPI ranks. Thus we would have 64 processes.


mpiexec -n 64 Ray -mini-ranks-per-rank 1

  (the default for -mini-ranks-per-rank is 1)

 - 8 nodes
 - 16 processors
 - 64 cores
 - 64 MPI ranks
 - 64 processes
 - 64 threads (1-to-1)

Case 2:
An alternative is to have 1 MPI rank per node. Each MPI rank has 7 miniranks.
Each minirank runs in a thread. There is one extra thread for communication too.

mpiexec -n 8 -bynode Ray -mini-ranks-per-rank 7

 - 8 nodes
 - 16 processors
 - 64 cores
 - 8 MPI ranks
 - 8 processes
 - 8*7=54 mini-rank threads
 - 8 comm. threads (1-to-1)

Case 3:
We can also use 1 MPI rank per processor, and 3 miniranks per rank.

mpiexec -n 16 -bynode Ray -mini-ranks-per-rank 3

 - 8 nodes
 - 16 processors
 - 64 cores
 - 16 MPI ranks
 - 16 processes
 - 16*3=48 mini-rank threads
 - 16*1=16 comm. threads (1-to-1)


With Open-MPI, the --bind-to-socket will be very nice.
The -bynode option is necessary.

Tasks:

- [DONE]add VirtualMachine
- [DONE] add MiniRank
- propagate mini rank numbers instead of ranks
- compile Ray without errors
- port plugin to the old adapter architecture
- implement the case with 1 minirank per rank (no pthread)
